{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The API I built for processing data and interacting with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    \"\"\"\n",
    "    Normalizes data\n",
    "    :param features: map of features to values \n",
    "    :return: normalized feature\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_features = {}\n",
    "    for key, val in features.items():\n",
    "        min_val = np.amin(val)\n",
    "        max_val = np.amax(val)\n",
    "        diff = max_val - min_val    \n",
    "        norm_features[key] = (val - min_val)/diff\n",
    "        \n",
    "    return norm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(features, dataframe):\n",
    "    \"\"\"\n",
    "    Extracts selected features from data\n",
    "    :param features: list of column names \n",
    "    :param dataframe: the dataframe to select features from\n",
    "    :return: map of selected features to numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_frame = {}\n",
    "    \n",
    "    # add features to feature_frame\n",
    "    for feature in features:\n",
    "        try:\n",
    "            # convert values to numpy array\n",
    "            feature_frame[feature] = np.array(dataframe[feature])\n",
    "        except:\n",
    "            print(\"feature not in dataframe\")\n",
    "            raise\n",
    "        \n",
    "    return feature_frame\n",
    "\n",
    "def process_categorical_data(feature,delimiter=None):\n",
    "    \"\"\"\n",
    "    convert categorical data to usable format\n",
    "    :param feature: panda series to be processed\n",
    "    :param delimiter: to separate multiple values for a sample's feature \n",
    "    :return: tensor with modified values\n",
    "    \"\"\"\n",
    "    \n",
    "    # create new tensors\n",
    "    values = []\n",
    "    for value in feature:\n",
    "        # in case number present\n",
    "        try:\n",
    "            new_value = value.split(delimiter)\n",
    "            values.append(new_value)\n",
    "        except:\n",
    "            values.append([value])\n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_feature(feature):\n",
    "    \"\"\"\n",
    "    Convert categorical data to numerical array using multihot-encoding\n",
    "    :param feature: list of categories\n",
    "    :return: numpy array of data, encoding scheme\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = len(feature)\n",
    "   \n",
    "    # index of 1's for each sample encoding\n",
    "    data_indices = []\n",
    "    \n",
    "    # encode each sample\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for sample in feature:\n",
    "        sample_indices = []\n",
    "        for category in sample:\n",
    "            if category not in vocab: \n",
    "                vocab[category] = index\n",
    "                index+=1\n",
    "            sample_indices.append(vocab[category])\n",
    "        data_indices.append(sample_indices)\n",
    "        \n",
    "    # create tensor and load in 1's\n",
    "    num_data = np.zeros((samples,index),dtype=np.float32)\n",
    "    row = 0\n",
    "    for sample_indices in data_indices:\n",
    "        for index in sample_indices:\n",
    "            num_data[row,index] = 1\n",
    "        row+=1\n",
    "        \n",
    "    return num_data, vocab\n",
    "\n",
    "def integrate_features(features):\n",
    "    \"\"\"\n",
    "    concatenates features into a big matrix\n",
    "    :param features: list of tensors\n",
    "    :return: np array\n",
    "    \"\"\"\n",
    "    return reduce(lambda x,y: np.column_stack((x,y)),features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_labels(targets,dataframe):\n",
    "    \"\"\"\n",
    "    Extracts selected labels from data\n",
    "    :param targets: list of target data\n",
    "    :param dataframe: the dataframe to select targets from\n",
    "    :return: dataframe of target data\n",
    "    \"\"\"\n",
    "    \n",
    "    target_frame = pd.DataFrame()\n",
    "    \n",
    "    # add targets to target_frame\n",
    "    for target in targets:\n",
    "        try:\n",
    "            target_frame[target] = dataframe[target]\n",
    "        except:\n",
    "            print(\"feature not in dataframe\")\n",
    "            raise\n",
    "        \n",
    "    return target_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_binarycolumns(labels,binary_values):\n",
    "    \"\"\"\n",
    "    Multihot encoding of multiple columns with binary values\n",
    "    :param labels: dataframe to be encoded\n",
    "    :param binary_values: dictionary mapping dataset binary values to a 1 or 0\n",
    "    :param new_column_name: name of encoded column\n",
    "    :return: numpy array\n",
    "    \"\"\"\n",
    "       \n",
    "    # create encoded column\n",
    "    data = np.zeros((labels.shape),dtype=np.float32)\n",
    "    \n",
    "    # convert binary values to 1's and 0's for each item    \n",
    "    for index, row in labels.iterrows():\n",
    "        \n",
    "        # weird bug where index can be equal to num rows\n",
    "        if index >= labels.shape[0]:\n",
    "            break\n",
    "        data[index] = row.apply(lambda death: binary_values[death])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(features,targets,batch_size = None):\n",
    "    \"\"\"\n",
    "    create batches to be fed into model\n",
    "    :param features: tensor of features\n",
    "    :param targets: tensor of targets\n",
    "    :param batch_size: desired size of batches\n",
    "    :param num_epochs: number of epochs\n",
    "    :return: batch iterator\n",
    "    \"\"\"\n",
    "    \n",
    "    # construct a dataset and configure batching/repeating\n",
    "    ds = Dataset.from_tensor_slices((features,targets)).shuffle(1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    # retrieve next batch\n",
    "    return ds.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special note: In tensorflow, a Graph contains the components of the model (the optimizer, the weights, the loss function, etc.) in the form of \"Tensors\" and \"Operations\". Later on, these objects will be activated in a separate step for training, called a Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_classifier(num_features,num_labels,optimizer_name, pred_type):\n",
    "    \"\"\"\n",
    "    creates a linear model\n",
    "    :param num_features: number of features to train on (NOT number of samples)\n",
    "    :param num_labels: number of target labels (NOT number of samples)\n",
    "    :param optimizer_name: name of optimizer\n",
    "    :param pred_type: the type of predictions being made (multi-class/multi-label)\n",
    "    :param name: name for graph's variable scope (must NOT be an empty string)\n",
    "    :return: tensorflow graph of net and list of relevant graph variables\n",
    "    \"\"\"\n",
    "        \n",
    "    # construct graph\n",
    "    graph = tf.Graph() \n",
    "    \n",
    "    # construct list of graph vars to run in session -- predictions, loss, optimizer, accuracy\n",
    "    graph_vars = {}\n",
    "    \n",
    "    # invoke tensorflow dataflow context\n",
    "    with graph.as_default():\n",
    "        \n",
    "        #with tf.variable_scope(name):\n",
    "            \n",
    "            # set up placeholder variables for input and output data\n",
    "            x_data = tf.placeholder(tf.float32,name=\"x_data\")\n",
    "            y_data = tf.placeholder(tf.float32,name=\"y_data\")\n",
    "            graph_vars['x_data'] = \"x_data:0\"\n",
    "            graph_vars['y_data'] = \"y_data:0\"\n",
    "            \n",
    "            # set up batching\n",
    "            batch_size = tf.placeholder(tf.int64,name=\"batch_size\")\n",
    "            iterator = create_batches(x_data,y_data,batch_size=batch_size)\n",
    "            init_iterator = tf.variables_initializer([iterator],name=\"iterator\") ## create iterator init op to reinitialize each epoch\n",
    "            graph_vars['iterator'] = \"iterator\"\n",
    "            \n",
    "            \n",
    "            # get next batch \n",
    "            batch_x_train, batch_y_train = iterator.get_next()\n",
    "            graph_vars['batch_size'] = \"batch_size:0\"\n",
    "            \n",
    "            # set up weights, biases, logits\n",
    "            weights = tf.Variable(tf.truncated_normal([num_features,num_labels]))\n",
    "            biases = tf.Variable(tf.zeros([num_labels]))\n",
    "            logits = tf.add(tf.matmul(batch_x_train,weights), biases)\n",
    "            \n",
    "            # apply appropriate transformation to logits and get loss\n",
    "            error_fn = select_error(pred_type)\n",
    "            loss = tf.reduce_mean(error_fn(logits = logits,labels = batch_y_train),name=\"loss\")\n",
    "            graph_vars['loss'] = \"loss:0\"\n",
    "            \n",
    "            # optimizer + backpropagation\n",
    "            learning_rate = tf.placeholder(tf.float32,name=\"learning_rate\")\n",
    "            optimizer = select_optimizer(optimizer_name,learning_rate)\n",
    "            back_propagation = optimizer.minimize(loss,name=\"back_propagation\")\n",
    "            graph_vars['back prop'] = \"back_propagation\"\n",
    "            graph_vars['learning_rate'] = \"learning_rate:0\"\n",
    "            \n",
    "            # calculate accuracy\n",
    "            predictions = tf.identity(get_predictions(pred_type,logits),name=\"predictions\")\n",
    "            equality = tf.equal(predictions,batch_y_train)\n",
    "            accuracy = tf.reduce_mean(tf.cast(equality,tf.float32),name=\"accuracy\")\n",
    "            graph_vars['accuracy'] = \"accuracy:0\"\n",
    "            graph_vars['predictions'] = \"predictions:0\"\n",
    "\n",
    "    return graph, graph_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vanilla_nn(num_features, num_labels, optimizer_name, pred_type, layers = None):\n",
    "    \"\"\"\n",
    "    creates a vanilla (aka fully connected feed forward) neural network\n",
    "    :param num_features: number of features to train on (NOT number of samples)\n",
    "    :param num_labels: number of target labels (NOT number of samples)\n",
    "    :param optimizer_name: name of optimizer\n",
    "    :param pred_type: the type of predictions being made (multi-class/multi-label)\n",
    "    :param layers: list of tuples for each layer (num hidden nodes, name of activation func). None = linear classifier\n",
    "    :param name: name for graph's scope\n",
    "    :return: tensorflow graph of net and list of relevant graph variables\n",
    "    \"\"\"\n",
    "    \n",
    "    if layers == None:\n",
    "        return create_linear_classifier(num_features,num_labels,optimizer_name, pred_type)\n",
    "            \n",
    "    # construct graph\n",
    "    graph = tf.Graph() \n",
    "    \n",
    "    # construct map of graph vars to run in session -- predictions, loss, optimizer, accuracy, data, etc.\n",
    "    graph_vars = {}\n",
    "        \n",
    "    # invoke tensorflow dataflow context\n",
    "    with graph.as_default():\n",
    "        \n",
    "        #with tf.variable_scope(name):\n",
    "        \n",
    "            # set up placeholder variables for input and output data\n",
    "            x_data = tf.placeholder(tf.float32,name=\"x_data\")\n",
    "            y_data = tf.placeholder(tf.float32,name=\"y_data\")\n",
    "            graph_vars['x_data'] = \"x_data:0\"\n",
    "            graph_vars['y_data'] = \"y_data:0\"\n",
    "            \n",
    "            # set up batching\n",
    "            batch_size = tf.placeholder(tf.int64,name=\"batch_size\")\n",
    "            iterator = create_batches(x_data,y_data,batch_size=batch_size)\n",
    "            init_iterator = tf.variables_initializer([iterator],name=\"iterator\") ## create iterator init op to reinitialize each epoch\n",
    "            graph_vars['iterator'] = \"iterator\"\n",
    "            \n",
    "            \n",
    "            # get next batch \n",
    "            batch_x_train, batch_y_train = iterator.get_next()\n",
    "            graph_vars['batch_size'] = \"batch_size:0\"\n",
    "                    \n",
    "            prev_rows = num_features \n",
    "            prev_layer = batch_x_train\n",
    "            num_layer = 1\n",
    "            \n",
    "            # create hidden layers\n",
    "            for layer in layers:\n",
    "                \n",
    "                # extract number of nodes in hidden layer and activation func\n",
    "                num_nodes = layer[0]\n",
    "                activation_func = select_activation_func(layer[1])\n",
    "                \n",
    "                # create layer\n",
    "                weights = tf.Variable(tf.truncated_normal([prev_rows,num_nodes]))\n",
    "                biases = tf.Variable(tf.zeros([num_nodes]))\n",
    "                hidden = activation_func(tf.add(tf.matmul(prev_layer,weights),biases))\n",
    "                graph_vars[\"hidden \"+str(num_layer)] = hidden\n",
    "                \n",
    "                num_layer+=1\n",
    "                prev_layer = hidden\n",
    "                \n",
    "                # update shape for next layer\n",
    "                prev_rows = num_nodes\n",
    "                          \n",
    "            # final layer\n",
    "            final = tf.Variable(tf.truncated_normal([prev_rows,num_labels]))\n",
    "            \n",
    "            # set up biases and logits\n",
    "            biases = tf.Variable(tf.zeros([num_labels]))\n",
    "            logits = tf.add(tf.matmul(prev_layer,final), biases)\n",
    "            \n",
    "            # apply appropriate transformation to logits and get loss\n",
    "            error_fn = select_error(pred_type)\n",
    "            loss = tf.reduce_mean(error_fn(logits = logits,labels = batch_y_train),name=\"loss\")\n",
    "            graph_vars['loss'] = \"loss:0\"\n",
    "            \n",
    "            # optimizer + backpropagation\n",
    "            learning_rate = tf.placeholder(tf.float32,name=\"learning_rate\")\n",
    "            optimizer = select_optimizer(optimizer_name,learning_rate)\n",
    "            back_propagation = optimizer.minimize(loss,name=\"back_propagation\")\n",
    "            graph_vars['back prop'] = \"back_propagation\"\n",
    "            graph_vars['learning_rate'] = \"learning_rate:0\"\n",
    "            \n",
    "            # calculate accuracy\n",
    "            predictions = tf.identity(get_predictions(pred_type,logits),name=\"predictions\")\n",
    "            equality = tf.equal(predictions,batch_y_train)\n",
    "            accuracy = tf.reduce_mean(tf.cast(equality,tf.float32),name=\"accuracy\")\n",
    "            graph_vars['accuracy'] = \"accuracy:0\"\n",
    "            graph_vars['predictions'] = \"predictions:0\"\n",
    "        \n",
    "    return graph, graph_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(name,learning_rate):\n",
    "    \"\"\"\n",
    "    Select user specified optimizer\n",
    "    :param name: name of optimizer\n",
    "    :param learning_rate: optimizer's learning rate\n",
    "    :return: optimizer object\n",
    "    \"\"\"\n",
    "    \n",
    "    name = name.lower()\n",
    "    \n",
    "    # list of optimizers    \n",
    "    optimizers = set([\"adam\",\"gd\",\"adagrad\",\"adadelta\"])\n",
    "    assert name in optimizers\n",
    "    \n",
    "    # TODO: make more memory efficient\n",
    "    return {\n",
    "        \"gd\" : tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
    "        \"adam\" : tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "        \"adagrad\" : tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "        \"adadelta\": tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "    }.get(name)\n",
    "\n",
    "def select_activation_func(name):\n",
    "    \"\"\"\n",
    "    Selects user specified activation fn\n",
    "    :param name: name of activation fn\n",
    "    :return: activation fn\n",
    "    \"\"\"\n",
    "    \n",
    "    name = name.lower()\n",
    "    \n",
    "    # list of activation fn\n",
    "    activations = set([\"sigmoid\",\"relu\",\"tanh\"])\n",
    "    assert name in activations\n",
    "    return {\n",
    "        \"sigmoid\" : tf.sigmoid,\n",
    "        \"relu\" : tf.nn.relu,\n",
    "        \"tanh\" : tf.tanh\n",
    "    }.get(name)\n",
    "\n",
    "\n",
    "def select_error(pred_type):\n",
    "    \"\"\"\n",
    "    Select type of error based on classification task\n",
    "    :param pred_type: type of predictions being made\n",
    "    :return: error op\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of classification types\n",
    "    types = set([\"multi-class\",\"multi-label\"])\n",
    "    assert pred_type in types\n",
    "      \n",
    "    return {\n",
    "        \"multi-class\" : tf.nn.softmax_cross_entropy_with_logits_v2,\n",
    "        \"multi-label\" : tf.nn.sigmoid_cross_entropy_with_logits\n",
    "    }.get(pred_type)\n",
    "\n",
    "def get_predictions(pred_type, logits):\n",
    "    \"\"\"\n",
    "    Calculate predictions based on classification task\n",
    "    :param pred_type: type of prediction being made\n",
    "    :param logits: unscaled predictions to be transformed\n",
    "    :return: predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of classification types\n",
    "    types = set([\"multi-class\",\"multi-label\"])\n",
    "    assert pred_type in types\n",
    "    \n",
    "    def multi_label_accuracy(logits):\n",
    "        return tf.round(tf.nn.sigmoid(logits))\n",
    "    \n",
    "    def multi_class_accuracy(logits):\n",
    "        return tf.round(tf.nn.softmax(logits))\n",
    "    \n",
    "    return {\n",
    "        \"multi-class\": multi_class_accuracy(logits),\n",
    "        \"multi-label\": multi_label_accuracy(logits)\n",
    "    }.get(pred_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special note: In low level tensorflow, training the model (which is a contained in a Graph) occurs in a Session. Here I also plot the accuracy and error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_data, y_data, model,model_vars, learning_rate = 0.1, epochs = 1, batch_size = None, save_path = None):\n",
    "    \"\"\"\n",
    "    train model and plot accuracy\n",
    "    :param x_data: input data\n",
    "    :param y_data: ground truth data\n",
    "    :param model: graph object containing net\n",
    "    :param model_vars: map of tensors/ops such as optimizer, loss, iterator etc. to run in session\n",
    "    :param learning_rate: the optimizer's learning rate in the model\n",
    "    :param epochs: number of epochs to train\n",
    "    :param batch_size: number of samples in each batch\n",
    "    :param save_path: file-path to save model if specified\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # lists of average accuracy and error for each epoch\n",
    "    accuracy_points = []\n",
    "    error_points = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # start session\n",
    "    with tf.Session(graph=model) as session:\n",
    "        \n",
    "        # initialize global variables\n",
    "        tf.global_variables_initializer().run()       \n",
    "        \n",
    "        # load data\n",
    "        feed_dict = {\n",
    "                        model_vars['x_data']: x_data,\n",
    "                        model_vars['y_data']: y_data,\n",
    "                        model_vars['batch_size']: batch_size if batch_size else x_data.shape[0],\n",
    "                    }\n",
    "        \n",
    "        # session variables\n",
    "        sess_vars = [model_vars['loss'],model_vars['back prop'],model_vars['accuracy']]\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # reset for each epoch\n",
    "            epoch_acc = 0.0\n",
    "            epoch_err = 0.0\n",
    "            denom = 0\n",
    "            session.run([model_vars['iterator']],feed_dict=feed_dict) # feed data into model\n",
    "            \n",
    "            # get batch until no batches left\n",
    "            while(True):\n",
    "                try:\n",
    "                    l, _, acc= session.run(sess_vars,feed_dict={model_vars['learning_rate']:learning_rate}) # feed in learning rate\n",
    "                    epoch_acc+=acc\n",
    "                    epoch_err+=l\n",
    "                    denom+=1\n",
    "                except:\n",
    "                    break\n",
    "                                \n",
    "            accuracy_points.append(epoch_acc/denom)\n",
    "            error_points.append(epoch_err/denom)\n",
    "        \n",
    "        # save if specified\n",
    "        if save_path:    \n",
    "            save_model(session,save_path)\n",
    "            \n",
    "        # plot data and end session\n",
    "        session.close()        \n",
    "        \n",
    "        x_values = np.arange(1,epochs+1)\n",
    "        fig, axes = plt.subplots(ncols=2)\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(10)\n",
    "        \n",
    "        axes[0].plot(x_values, accuracy_points, 'b-')\n",
    "        axes[0].set_ylim(0.0,1.0)\n",
    "        axes[0].set_title(\"Average Accuracy/epoch\")\n",
    "        axes[0].set_xlabel(\"Epochs\")\n",
    "        axes[0].set_ylabel(\"Accuracy (%)\")\n",
    "        \n",
    "        axes[1].plot(x_values,error_points,'r-')\n",
    "        axes[1].set_title(\"Loss/epoch\")\n",
    "        axes[1].set_xlabel(\"Epochs\")\n",
    "        axes[1].set_ylabel(\"Loss\")\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x_data,y_data, model_vars, restore_paths,vocab=None):\n",
    "    \"\"\"\n",
    "    tests model on test data\n",
    "    :param x_data: new feature data to perform predictions \n",
    "    :param y_data: ground truth data\n",
    "    :param model_vars: map of tensors/ops such as optimizer, loss, iterator etc. to run in session\n",
    "    :param restore_paths: map of path names to paths\n",
    "    :param vocab: optional parameter to decode predictions if necessary\n",
    "    :return: predictions, accuracy, and loss\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "                \n",
    "        # restore model in this session\n",
    "        restore_model(session,restore_paths['metagraph'], restore_paths['checkpoint'])     \n",
    "               \n",
    "        # load data        \n",
    "        feed_dict = {\n",
    "                        model_vars['x_data']: x_data,\n",
    "                        model_vars['y_data']: y_data,\n",
    "                        model_vars['batch_size']: x_data.shape[0],\n",
    "                    }\n",
    "        \n",
    "        # feed in test data and extract results\n",
    "        session.run(model_vars['iterator'],feed_dict=feed_dict)\n",
    "        sess_vars = [model_vars['accuracy'],model_vars['loss'],model_vars['predictions']]\n",
    "        acc, loss, preds = session.run(sess_vars, feed_dict=feed_dict)\n",
    "                        \n",
    "        # terminate session\n",
    "        session.close()\n",
    "        return preds, acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(sess, save_path):\n",
    "    \"\"\"\n",
    "    Saves model\n",
    "    :param sess: session object containing graph variables in trained state\n",
    "    :param save_path: file-path to save session graph\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(sess, meta_graph_path, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Restores model in current session\n",
    "    :param sess: session object to restore model under\n",
    "    :param meta_graph_path: path to meta graph file created when saving model\n",
    "    :param checkpoint_path: path to checkpoint file for current notebook\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    saver = tf.train.import_meta_graph(meta_graph_path)\n",
    "    saver.restore(sess,checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}